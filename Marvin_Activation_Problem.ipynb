{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "rSupZ",
      "launcher_item_id": "cvGhe"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Marvin Activation Problem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaynadimpalli/MarvinVoiceActivation/blob/main/Marvin_Activation_Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b410C5FIQsgr"
      },
      "source": [
        "#https://stackoverflow.com/questions/48309631/tensorflow-tf-data-dataset-reading-large-hdf5-files\n",
        "\n",
        "#https://stackoverflow.com/questions/47059698/keras-custom-data-generator-for-large-hdf5-file-which-does-not-fit-into-memory"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ETA7v2SbsQX",
        "outputId": "583d7c43-56cb-4935-dfbe-1b1636d26d3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pydub\n",
        "\n",
        "!pip install wget\n",
        "\n",
        "!pip install gitpython\n",
        "\n",
        "!pip install ffmpeg-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.24.1\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=67976e5a20155105daad8d8d49a14a49f1174dee9302e45e7b23e00d936cbf3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/d1/a7f8fe3df258549b303415157328bfcc63e9b11d06a7ad7a3327f3d32606/GitPython-3.1.11-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 7.1MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.5 gitpython-3.1.11 smmap-3.0.4\n",
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vjBoslieprb"
      },
      "source": [
        "import git\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "t = tempfile.mkdtemp()\n",
        "# Clone into temporary dir\n",
        "git.Repo.clone_from('https://github.com/vijaynadimpalli/MarvinVoiceActivation.git', t)\n",
        "# Copy desired file from temporary dir\n",
        "shutil.move(os.path.join(t, 'DataCreator.py'), '.')\n",
        "shutil.move(os.path.join(t, 'ValidationHelperFunctions.py'), '.')\n",
        "# Remove temporary dir\n",
        "shutil.rmtree(t)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld65QYkNfisD",
        "outputId": "c0cf639f-4475-4653-e282-1a0edfb0f2b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "\n",
        "import DataCreator\n",
        "from DataCreator import *\n",
        "from ValidationHelperFunctions import *\n",
        "\n",
        "num_actives = 500\n",
        "num_negatives = 500\n",
        "train_examples = 6400\n",
        "val_examples = 1280\n",
        "chunk_size = 16\n",
        "silent_background = 1  # 1 == True\n",
        "\n",
        "#Sampling rate of training data is 16000Hz\n",
        "\n",
        "#takes about 20 min to run for 15000 examples\n",
        "\n",
        "!python DataCreator.py $num_actives $num_negatives $train_examples $val_examples $chunk_size $silent_background #for jupyter and ipython to resolve the lingering RAM issue with ipython due to huge amount of png files.....\n",
        "#DataCreator.main([num_actives, num_negatives, train_examples, val_examples, chunk_size]) #for pycharm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py:7592: RuntimeWarning: divide by zero encountered in log10\n",
            "  Z = 10. * np.log10(spec)\n",
            "0.0 Chunk stored\n",
            "1.0 Chunk stored\n",
            "2.0 Chunk stored\n",
            "3.0 Chunk stored\n",
            "4.0 Chunk stored\n",
            "5.0 Chunk stored\n",
            "6.0 Chunk stored\n",
            "7.0 Chunk stored\n",
            "8.0 Chunk stored\n",
            "9.0 Chunk stored\n",
            "10.0 Chunk stored\n",
            "11.0 Chunk stored\n",
            "12.0 Chunk stored\n",
            "13.0 Chunk stored\n",
            "14.0 Chunk stored\n",
            "15.0 Chunk stored\n",
            "16.0 Chunk stored\n",
            "17.0 Chunk stored\n",
            "18.0 Chunk stored\n",
            "19.0 Chunk stored\n",
            "20.0 Chunk stored\n",
            "21.0 Chunk stored\n",
            "22.0 Chunk stored\n",
            "23.0 Chunk stored\n",
            "24.0 Chunk stored\n",
            "25.0 Chunk stored\n",
            "26.0 Chunk stored\n",
            "27.0 Chunk stored\n",
            "28.0 Chunk stored\n",
            "29.0 Chunk stored\n",
            "30.0 Chunk stored\n",
            "31.0 Chunk stored\n",
            "32.0 Chunk stored\n",
            "33.0 Chunk stored\n",
            "34.0 Chunk stored\n",
            "35.0 Chunk stored\n",
            "36.0 Chunk stored\n",
            "37.0 Chunk stored\n",
            "38.0 Chunk stored\n",
            "39.0 Chunk stored\n",
            "40.0 Chunk stored\n",
            "41.0 Chunk stored\n",
            "42.0 Chunk stored\n",
            "43.0 Chunk stored\n",
            "44.0 Chunk stored\n",
            "45.0 Chunk stored\n",
            "46.0 Chunk stored\n",
            "47.0 Chunk stored\n",
            "48.0 Chunk stored\n",
            "49.0 Chunk stored\n",
            "50.0 Chunk stored\n",
            "51.0 Chunk stored\n",
            "52.0 Chunk stored\n",
            "53.0 Chunk stored\n",
            "54.0 Chunk stored\n",
            "55.0 Chunk stored\n",
            "56.0 Chunk stored\n",
            "57.0 Chunk stored\n",
            "58.0 Chunk stored\n",
            "59.0 Chunk stored\n",
            "60.0 Chunk stored\n",
            "61.0 Chunk stored\n",
            "62.0 Chunk stored\n",
            "63.0 Chunk stored\n",
            "64.0 Chunk stored\n",
            "65.0 Chunk stored\n",
            "66.0 Chunk stored\n",
            "67.0 Chunk stored\n",
            "68.0 Chunk stored\n",
            "69.0 Chunk stored\n",
            "70.0 Chunk stored\n",
            "71.0 Chunk stored\n",
            "72.0 Chunk stored\n",
            "73.0 Chunk stored\n",
            "74.0 Chunk stored\n",
            "75.0 Chunk stored\n",
            "76.0 Chunk stored\n",
            "77.0 Chunk stored\n",
            "78.0 Chunk stored\n",
            "79.0 Chunk stored\n",
            "80.0 Chunk stored\n",
            "81.0 Chunk stored\n",
            "82.0 Chunk stored\n",
            "83.0 Chunk stored\n",
            "84.0 Chunk stored\n",
            "85.0 Chunk stored\n",
            "86.0 Chunk stored\n",
            "87.0 Chunk stored\n",
            "88.0 Chunk stored\n",
            "89.0 Chunk stored\n",
            "90.0 Chunk stored\n",
            "91.0 Chunk stored\n",
            "92.0 Chunk stored\n",
            "93.0 Chunk stored\n",
            "94.0 Chunk stored\n",
            "95.0 Chunk stored\n",
            "96.0 Chunk stored\n",
            "97.0 Chunk stored\n",
            "98.0 Chunk stored\n",
            "99.0 Chunk stored\n",
            "100.0 Chunk stored\n",
            "101.0 Chunk stored\n",
            "102.0 Chunk stored\n",
            "103.0 Chunk stored\n",
            "104.0 Chunk stored\n",
            "105.0 Chunk stored\n",
            "106.0 Chunk stored\n",
            "107.0 Chunk stored\n",
            "108.0 Chunk stored\n",
            "109.0 Chunk stored\n",
            "110.0 Chunk stored\n",
            "111.0 Chunk stored\n",
            "112.0 Chunk stored\n",
            "113.0 Chunk stored\n",
            "114.0 Chunk stored\n",
            "115.0 Chunk stored\n",
            "116.0 Chunk stored\n",
            "117.0 Chunk stored\n",
            "118.0 Chunk stored\n",
            "119.0 Chunk stored\n",
            "120.0 Chunk stored\n",
            "121.0 Chunk stored\n",
            "122.0 Chunk stored\n",
            "123.0 Chunk stored\n",
            "124.0 Chunk stored\n",
            "125.0 Chunk stored\n",
            "126.0 Chunk stored\n",
            "127.0 Chunk stored\n",
            "128.0 Chunk stored\n",
            "129.0 Chunk stored\n",
            "130.0 Chunk stored\n",
            "131.0 Chunk stored\n",
            "132.0 Chunk stored\n",
            "133.0 Chunk stored\n",
            "134.0 Chunk stored\n",
            "135.0 Chunk stored\n",
            "136.0 Chunk stored\n",
            "137.0 Chunk stored\n",
            "138.0 Chunk stored\n",
            "139.0 Chunk stored\n",
            "140.0 Chunk stored\n",
            "141.0 Chunk stored\n",
            "142.0 Chunk stored\n",
            "143.0 Chunk stored\n",
            "144.0 Chunk stored\n",
            "145.0 Chunk stored\n",
            "146.0 Chunk stored\n",
            "147.0 Chunk stored\n",
            "148.0 Chunk stored\n",
            "149.0 Chunk stored\n",
            "150.0 Chunk stored\n",
            "151.0 Chunk stored\n",
            "152.0 Chunk stored\n",
            "153.0 Chunk stored\n",
            "154.0 Chunk stored\n",
            "155.0 Chunk stored\n",
            "156.0 Chunk stored\n",
            "157.0 Chunk stored\n",
            "158.0 Chunk stored\n",
            "159.0 Chunk stored\n",
            "160.0 Chunk stored\n",
            "161.0 Chunk stored\n",
            "162.0 Chunk stored\n",
            "163.0 Chunk stored\n",
            "164.0 Chunk stored\n",
            "165.0 Chunk stored\n",
            "166.0 Chunk stored\n",
            "167.0 Chunk stored\n",
            "168.0 Chunk stored\n",
            "169.0 Chunk stored\n",
            "170.0 Chunk stored\n",
            "171.0 Chunk stored\n",
            "172.0 Chunk stored\n",
            "173.0 Chunk stored\n",
            "174.0 Chunk stored\n",
            "175.0 Chunk stored\n",
            "176.0 Chunk stored\n",
            "177.0 Chunk stored\n",
            "178.0 Chunk stored\n",
            "179.0 Chunk stored\n",
            "180.0 Chunk stored\n",
            "181.0 Chunk stored\n",
            "182.0 Chunk stored\n",
            "183.0 Chunk stored\n",
            "184.0 Chunk stored\n",
            "185.0 Chunk stored\n",
            "186.0 Chunk stored\n",
            "187.0 Chunk stored\n",
            "188.0 Chunk stored\n",
            "189.0 Chunk stored\n",
            "190.0 Chunk stored\n",
            "191.0 Chunk stored\n",
            "192.0 Chunk stored\n",
            "193.0 Chunk stored\n",
            "194.0 Chunk stored\n",
            "195.0 Chunk stored\n",
            "196.0 Chunk stored\n",
            "197.0 Chunk stored\n",
            "198.0 Chunk stored\n",
            "199.0 Chunk stored\n",
            "200.0 Chunk stored\n",
            "201.0 Chunk stored\n",
            "202.0 Chunk stored\n",
            "203.0 Chunk stored\n",
            "204.0 Chunk stored\n",
            "205.0 Chunk stored\n",
            "206.0 Chunk stored\n",
            "207.0 Chunk stored\n",
            "208.0 Chunk stored\n",
            "209.0 Chunk stored\n",
            "210.0 Chunk stored\n",
            "211.0 Chunk stored\n",
            "212.0 Chunk stored\n",
            "213.0 Chunk stored\n",
            "214.0 Chunk stored\n",
            "215.0 Chunk stored\n",
            "216.0 Chunk stored\n",
            "217.0 Chunk stored\n",
            "218.0 Chunk stored\n",
            "219.0 Chunk stored\n",
            "220.0 Chunk stored\n",
            "221.0 Chunk stored\n",
            "222.0 Chunk stored\n",
            "223.0 Chunk stored\n",
            "224.0 Chunk stored\n",
            "225.0 Chunk stored\n",
            "226.0 Chunk stored\n",
            "227.0 Chunk stored\n",
            "228.0 Chunk stored\n",
            "229.0 Chunk stored\n",
            "230.0 Chunk stored\n",
            "231.0 Chunk stored\n",
            "232.0 Chunk stored\n",
            "233.0 Chunk stored\n",
            "234.0 Chunk stored\n",
            "235.0 Chunk stored\n",
            "236.0 Chunk stored\n",
            "237.0 Chunk stored\n",
            "238.0 Chunk stored\n",
            "239.0 Chunk stored\n",
            "240.0 Chunk stored\n",
            "241.0 Chunk stored\n",
            "242.0 Chunk stored\n",
            "243.0 Chunk stored\n",
            "244.0 Chunk stored\n",
            "245.0 Chunk stored\n",
            "246.0 Chunk stored\n",
            "247.0 Chunk stored\n",
            "248.0 Chunk stored\n",
            "249.0 Chunk stored\n",
            "250.0 Chunk stored\n",
            "251.0 Chunk stored\n",
            "252.0 Chunk stored\n",
            "253.0 Chunk stored\n",
            "254.0 Chunk stored\n",
            "255.0 Chunk stored\n",
            "256.0 Chunk stored\n",
            "257.0 Chunk stored\n",
            "258.0 Chunk stored\n",
            "259.0 Chunk stored\n",
            "260.0 Chunk stored\n",
            "261.0 Chunk stored\n",
            "262.0 Chunk stored\n",
            "263.0 Chunk stored\n",
            "264.0 Chunk stored\n",
            "265.0 Chunk stored\n",
            "266.0 Chunk stored\n",
            "267.0 Chunk stored\n",
            "268.0 Chunk stored\n",
            "269.0 Chunk stored\n",
            "270.0 Chunk stored\n",
            "271.0 Chunk stored\n",
            "272.0 Chunk stored\n",
            "273.0 Chunk stored\n",
            "274.0 Chunk stored\n",
            "275.0 Chunk stored\n",
            "276.0 Chunk stored\n",
            "277.0 Chunk stored\n",
            "278.0 Chunk stored\n",
            "279.0 Chunk stored\n",
            "280.0 Chunk stored\n",
            "281.0 Chunk stored\n",
            "282.0 Chunk stored\n",
            "283.0 Chunk stored\n",
            "284.0 Chunk stored\n",
            "285.0 Chunk stored\n",
            "286.0 Chunk stored\n",
            "287.0 Chunk stored\n",
            "288.0 Chunk stored\n",
            "289.0 Chunk stored\n",
            "290.0 Chunk stored\n",
            "291.0 Chunk stored\n",
            "292.0 Chunk stored\n",
            "293.0 Chunk stored\n",
            "294.0 Chunk stored\n",
            "295.0 Chunk stored\n",
            "296.0 Chunk stored\n",
            "297.0 Chunk stored\n",
            "298.0 Chunk stored\n",
            "299.0 Chunk stored\n",
            "300.0 Chunk stored\n",
            "301.0 Chunk stored\n",
            "302.0 Chunk stored\n",
            "303.0 Chunk stored\n",
            "304.0 Chunk stored\n",
            "305.0 Chunk stored\n",
            "306.0 Chunk stored\n",
            "307.0 Chunk stored\n",
            "308.0 Chunk stored\n",
            "309.0 Chunk stored\n",
            "310.0 Chunk stored\n",
            "311.0 Chunk stored\n",
            "312.0 Chunk stored\n",
            "313.0 Chunk stored\n",
            "314.0 Chunk stored\n",
            "315.0 Chunk stored\n",
            "316.0 Chunk stored\n",
            "317.0 Chunk stored\n",
            "318.0 Chunk stored\n",
            "319.0 Chunk stored\n",
            "320.0 Chunk stored\n",
            "321.0 Chunk stored\n",
            "322.0 Chunk stored\n",
            "323.0 Chunk stored\n",
            "324.0 Chunk stored\n",
            "325.0 Chunk stored\n",
            "326.0 Chunk stored\n",
            "327.0 Chunk stored\n",
            "328.0 Chunk stored\n",
            "329.0 Chunk stored\n",
            "330.0 Chunk stored\n",
            "331.0 Chunk stored\n",
            "332.0 Chunk stored\n",
            "333.0 Chunk stored\n",
            "334.0 Chunk stored\n",
            "335.0 Chunk stored\n",
            "336.0 Chunk stored\n",
            "337.0 Chunk stored\n",
            "338.0 Chunk stored\n",
            "339.0 Chunk stored\n",
            "340.0 Chunk stored\n",
            "341.0 Chunk stored\n",
            "342.0 Chunk stored\n",
            "343.0 Chunk stored\n",
            "344.0 Chunk stored\n",
            "345.0 Chunk stored\n",
            "346.0 Chunk stored\n",
            "347.0 Chunk stored\n",
            "348.0 Chunk stored\n",
            "349.0 Chunk stored\n",
            "350.0 Chunk stored\n",
            "351.0 Chunk stored\n",
            "352.0 Chunk stored\n",
            "353.0 Chunk stored\n",
            "354.0 Chunk stored\n",
            "355.0 Chunk stored\n",
            "356.0 Chunk stored\n",
            "357.0 Chunk stored\n",
            "358.0 Chunk stored\n",
            "359.0 Chunk stored\n",
            "360.0 Chunk stored\n",
            "361.0 Chunk stored\n",
            "362.0 Chunk stored\n",
            "363.0 Chunk stored\n",
            "364.0 Chunk stored\n",
            "365.0 Chunk stored\n",
            "366.0 Chunk stored\n",
            "367.0 Chunk stored\n",
            "368.0 Chunk stored\n",
            "369.0 Chunk stored\n",
            "370.0 Chunk stored\n",
            "371.0 Chunk stored\n",
            "372.0 Chunk stored\n",
            "373.0 Chunk stored\n",
            "374.0 Chunk stored\n",
            "375.0 Chunk stored\n",
            "376.0 Chunk stored\n",
            "377.0 Chunk stored\n",
            "378.0 Chunk stored\n",
            "379.0 Chunk stored\n",
            "380.0 Chunk stored\n",
            "381.0 Chunk stored\n",
            "382.0 Chunk stored\n",
            "383.0 Chunk stored\n",
            "384.0 Chunk stored\n",
            "385.0 Chunk stored\n",
            "386.0 Chunk stored\n",
            "387.0 Chunk stored\n",
            "388.0 Chunk stored\n",
            "389.0 Chunk stored\n",
            "390.0 Chunk stored\n",
            "391.0 Chunk stored\n",
            "392.0 Chunk stored\n",
            "393.0 Chunk stored\n",
            "394.0 Chunk stored\n",
            "395.0 Chunk stored\n",
            "396.0 Chunk stored\n",
            "397.0 Chunk stored\n",
            "398.0 Chunk stored\n",
            "399.0 Chunk stored\n",
            "0.0 Chunk stored\n",
            "1.0 Chunk stored\n",
            "2.0 Chunk stored\n",
            "3.0 Chunk stored\n",
            "4.0 Chunk stored\n",
            "5.0 Chunk stored\n",
            "6.0 Chunk stored\n",
            "7.0 Chunk stored\n",
            "8.0 Chunk stored\n",
            "9.0 Chunk stored\n",
            "10.0 Chunk stored\n",
            "11.0 Chunk stored\n",
            "12.0 Chunk stored\n",
            "13.0 Chunk stored\n",
            "14.0 Chunk stored\n",
            "15.0 Chunk stored\n",
            "16.0 Chunk stored\n",
            "17.0 Chunk stored\n",
            "18.0 Chunk stored\n",
            "19.0 Chunk stored\n",
            "20.0 Chunk stored\n",
            "21.0 Chunk stored\n",
            "22.0 Chunk stored\n",
            "23.0 Chunk stored\n",
            "24.0 Chunk stored\n",
            "25.0 Chunk stored\n",
            "26.0 Chunk stored\n",
            "27.0 Chunk stored\n",
            "28.0 Chunk stored\n",
            "29.0 Chunk stored\n",
            "30.0 Chunk stored\n",
            "31.0 Chunk stored\n",
            "32.0 Chunk stored\n",
            "33.0 Chunk stored\n",
            "34.0 Chunk stored\n",
            "35.0 Chunk stored\n",
            "36.0 Chunk stored\n",
            "37.0 Chunk stored\n",
            "38.0 Chunk stored\n",
            "39.0 Chunk stored\n",
            "40.0 Chunk stored\n",
            "41.0 Chunk stored\n",
            "42.0 Chunk stored\n",
            "43.0 Chunk stored\n",
            "44.0 Chunk stored\n",
            "45.0 Chunk stored\n",
            "46.0 Chunk stored\n",
            "47.0 Chunk stored\n",
            "48.0 Chunk stored\n",
            "49.0 Chunk stored\n",
            "50.0 Chunk stored\n",
            "51.0 Chunk stored\n",
            "52.0 Chunk stored\n",
            "53.0 Chunk stored\n",
            "54.0 Chunk stored\n",
            "55.0 Chunk stored\n",
            "56.0 Chunk stored\n",
            "57.0 Chunk stored\n",
            "58.0 Chunk stored\n",
            "59.0 Chunk stored\n",
            "60.0 Chunk stored\n",
            "61.0 Chunk stored\n",
            "62.0 Chunk stored\n",
            "63.0 Chunk stored\n",
            "64.0 Chunk stored\n",
            "65.0 Chunk stored\n",
            "66.0 Chunk stored\n",
            "67.0 Chunk stored\n",
            "68.0 Chunk stored\n",
            "69.0 Chunk stored\n",
            "70.0 Chunk stored\n",
            "71.0 Chunk stored\n",
            "72.0 Chunk stored\n",
            "73.0 Chunk stored\n",
            "74.0 Chunk stored\n",
            "75.0 Chunk stored\n",
            "76.0 Chunk stored\n",
            "77.0 Chunk stored\n",
            "78.0 Chunk stored\n",
            "79.0 Chunk stored\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIcS6Hz-oQFq",
        "cellView": "form"
      },
      "source": [
        "#@title Commented code for tf.dataset generation using python data generators\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "\n",
        "# x_shape = (32, 32, 3)\n",
        "# y_shape = ()  # A single item (not array).\n",
        "# classes = 10\n",
        "\n",
        "# def generator_fn(n_samples):\n",
        "#     \"\"\"Return a function that takes no arguments and returns a generator.\"\"\"\n",
        "#     def generator():\n",
        "#         for i in range(n_samples):\n",
        "#             # Synthesize an image and a class label.\n",
        "#             x = np.random.random_sample(x_shape).astype(np.float32)\n",
        "#             y = np.random.randint(0, classes, size=y_shape, dtype=np.int32)\n",
        "#             yield x, y\n",
        "#     return generator\n",
        "\n",
        "# def augment(x, y):\n",
        "#     return x * tf.random.normal(shape=x_shape), y\n",
        "\n",
        "# samples = 10\n",
        "# batch_size = 5\n",
        "# epochs = 2\n",
        "\n",
        "# # Create dataset.\n",
        "# gen = generator_fn(n_samples=samples)\n",
        "# dataset = tf.data.Dataset.from_generator(\n",
        "#     generator=gen, \n",
        "#     output_types=(np.float32, np.int32), \n",
        "#     output_shapes=(x_shape, y_shape)\n",
        "# )\n",
        "# # Parallelize the augmentation.\n",
        "# dataset = dataset.map(\n",
        "#     augment, \n",
        "#     num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "#     # Order does not matter.\n",
        "#     deterministic=False\n",
        "# )\n",
        "# dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "# # Prefetch some batches.\n",
        "# dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# # Prepare model.\n",
        "# model = tf.keras.applications.VGG16(weights=None, input_shape=x_shape, classes=classes)\n",
        "# model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "# # Train. Do not specify batch size because the dataset takes care of that.\n",
        "# model.fit(dataset, epochs=epochs)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9-HVZ8JlJjp",
        "cellView": "form"
      },
      "source": [
        "#@title Relevant Generator function for Dataset method\n",
        "# def generator_fn():\n",
        "#     \"\"\"Return a function that takes no arguments and returns a generator.\"\"\"\n",
        "#     def generator():\n",
        "#         while True:\n",
        "#             # Synthesize an image and a class label.\n",
        "#             x,y = create_training_example(backgrounds, activates, negatives)\n",
        "#             yield x.T, y.T\n",
        "#     return generator\n",
        "\n",
        "# \"\"\"\n",
        "# x_shape = (Tx,n_freq)\n",
        "# y_shape = (Ty,1)\n",
        "\n",
        "# # Create dataset.\n",
        "# gen = generator_fn()\n",
        "# dataset = tf.data.Dataset.from_generator(\n",
        "#     generator=gen, \n",
        "#     output_types=(np.float32, np.float32), \n",
        "#     output_shapes=(x_shape, y_shape)\n",
        "# )\n",
        "# dataset = dataset.batch(32, drop_remainder=True)\n",
        "# # Prefetch some batches.\n",
        "# dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "# hist = model.fit(dataset, epochs=10,steps_per_epoch=100,callbacks=[checkpointer])\n",
        "\n",
        "# \"\"\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIhf2TlF-mOt",
        "cellView": "both"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pydub import AudioSegment\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import IPython\n",
        "import tensorflow as tf\n",
        "import scipy\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
        "from tensorflow.keras.layers import GRU, Bidirectional, BatchNormalization, Reshape, RNN, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix,f1_score\n",
        "import pandas"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJdpa13eH2S"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self,x,y, batch_size=32, chunk_size = 16, shuffle=True):\n",
        "        self.batch_size = batch_size\n",
        "        self.chunk_size = chunk_size\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.indices = np.arange(0,x.shape[0],chunk_size)\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.floor(self.x.shape[0]/self.batch_size).astype('int')\n",
        "\n",
        "    def __getitem__(self, index): #input is batch number in a particular epoch\n",
        "        index = self.index[index * np.floor(self.batch_size/self.chunk_size).astype('int'):(index + 1) * np.floor(self.batch_size/self.chunk_size).astype('int')]\n",
        "        start_locs = [self.indices[k] for k in index]\n",
        "        \n",
        "        X, y = self.__get_data(start_locs)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.index = np.arange(len(self.indices))\n",
        "        if self.shuffle == True:          \n",
        "          #print(\"Shuffling\")\n",
        "          np.random.shuffle(self.index)\n",
        "\n",
        "    def __get_data(self, start_locs):\n",
        "        X = np.zeros((self.batch_size,Tx,n_freq))\n",
        "        y = np.zeros((self.batch_size,Ty,1))\n",
        "        \n",
        "        for i, start in enumerate(start_locs):\n",
        "          X[i:i+self.chunk_size,:,:] = np.array(self.x[start:start+self.chunk_size,:,:])\n",
        "          y[i:i+self.chunk_size,:,:] = np.array(self.y[start:start+self.chunk_size,:,:])\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XvuHmqLIAH3"
      },
      "source": [
        "f1 = h5py.File(\"./XY_train/XY.h5\",'r')\n",
        "f2 = h5py.File(\"./XY_dev/XY_dev.h5\",'r')\n",
        "batch_size = 256\n",
        "\n",
        "train_generator = DataGenerator(f1['X_train'],f1['Y_train'],batch_size=batch_size,chunk_size=chunk_size)\n",
        "test_generator = DataGenerator(f2['X_test'],f2['Y_test'],batch_size=batch_size,chunk_size=chunk_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZsD4BoHug04"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "\n",
        "\n",
        "#Another F1 function implementation(Using inbuilt metrics).....\n",
        "pres = tf.keras.metrics.Precision()\n",
        "rec = tf.keras.metrics.Recall()  \n",
        "\n",
        "def get_f1(y_true,y_pred):\n",
        "\n",
        "  #pres.reset_states()             #Using these reset_states gives same values as given by f1_m function\n",
        "  #rec.reset_states()              #reset_states clears any persistence,and clears it for every batch,giving same functionality as f1_m\n",
        "  pres.update_state(y_true,y_pred) #Not using reset_states averages out the f1 values over all batches in an epoch and doesn't reset with new epoch...\n",
        "  rec.update_state(y_true,y_pred)\n",
        "\n",
        "  return 2*pres.result()*rec.result()/(pres.result()+rec.result()+K.epsilon())\n",
        "\n",
        "\n",
        "class reset_states_f1(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epoch,logs=[]):\n",
        "    print(\"Resetting states of get_f1 metric\")\n",
        "    pres.reset_states()\n",
        "    rec.reset_states()\n",
        "\n",
        "  def on_test_batch_begin(self,batch,logs=[]):\n",
        "    if batch==0:\n",
        "      print(\"Resetting states of get_f1 metric at start of validation batches\")\n",
        "      pres.reset_states()\n",
        "      rec.reset_states()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhayKSEueacX",
        "outputId": "986a13aa-ad26-4885-8e23-b08e8ccfcbc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def model(input_shape):\n",
        "    X_input = Input(shape = input_shape)\n",
        "   \n",
        "    X = Conv1D(filters=196,kernel_size=15,strides=4)(X_input)                                 \n",
        "    X = BatchNormalization()(X)                                 \n",
        "    X = Activation('relu')(X)                                 \n",
        "    X = Dropout(0.8)(X)\n",
        "   \n",
        "    X = GRU(128,return_sequences=True)(X)                                \n",
        "    X = Dropout(0.8)(X)                                       \n",
        "    X = BatchNormalization()(X)\n",
        "   \n",
        "    X = GRU(128,return_sequences=True)(X)                                 \n",
        "    X = Dropout(0.8)(X)                                 \n",
        "    X = BatchNormalization()(X)                                 \n",
        "    X = Dropout(0.8)(X)\n",
        "\n",
        "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X)\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = model(input_shape = (Tx, n_freq))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "opt = Adam(lr=1e-2)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\", get_f1,f1_m])\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lambda epoch: 1e-5*(10**(epoch/40)))\n",
        "checkpointer = ModelCheckpoint('./model/',save_best_only=True,monitor='val_get_f1',mode='max',save_weights_only=True,verbose=0)\n",
        "\n",
        "hist = model.fit(train_generator,validation_data=test_generator,epochs=120,callbacks=[checkpointer,reset_states_f1()])\n",
        "\n",
        "\n",
        "model.load_weights('./model/')\n",
        "model.save(\"best_model&weights.h5\")\n",
        "\n",
        "chime_file = \"./chime.wav\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 1998, 101)]       0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 496, 196)          297136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 496, 196)          784       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 496, 196)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 496, 196)          0         \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 496, 128)          125184    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 496, 128)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 496, 128)          512       \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 496, 128)          99072     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 496, 128)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 496, 128)          512       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 496, 128)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 496, 1)            129       \n",
            "=================================================================\n",
            "Total params: 523,329\n",
            "Trainable params: 522,425\n",
            "Non-trainable params: 904\n",
            "_________________________________________________________________\n",
            "Epoch 1/120\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.8929 - get_f1: 0.0792 - f1_m: 0.1158Resetting states of get_f1 metric at start of validation batches\n",
            "Resetting states of get_f1 metric\n",
            "25/25 [==============================] - 99s 4s/step - loss: 0.4558 - accuracy: 0.8929 - get_f1: 0.0792 - f1_m: 0.1158 - val_loss: 0.1291 - val_accuracy: 0.9770 - val_get_f1: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
            "Epoch 2/120\n",
            "20/25 [=======================>......] - ETA: 6s - loss: 0.1649 - accuracy: 0.9720 - get_f1: 0.0950 - f1_m: 0.0761"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwgCZCBdhJjH"
      },
      "source": [
        "#plt.semilogx(hist.history['lr'],hist.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBOnQK_kDE8f"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.plot(hist.history['loss'],label='loss')\n",
        "\n",
        "plt.plot(hist.history['val_loss'],label='val_loss')\n",
        "\n",
        "plt.plot(hist.history['get_f1'],label='get_f1')\n",
        "\n",
        "plt.plot(hist.history['f1_m'],label='f1_m')\n",
        "\n",
        "plt.plot(hist.history['val_get_f1'],label='val_get_f1')\n",
        "\n",
        "plt.plot(hist.history['val_f1_m'],label='val_f1_m')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBEmbFD_HTep"
      },
      "source": [
        "activates, negatives, backgrounds = load_raw_audio(silent_background)\n",
        "create_training_example(backgrounds, activates, negatives)\n",
        "filename = \"./train.wav\"\n",
        "\n",
        "new_model = load_model(\"best_model&weights.h5\",custom_objects={'get_f1':get_f1,'f1_m':f1_m})\n",
        "prediction = detect_triggerword(filename,new_model)\n",
        "chime_on_activate(filename,chime_file, prediction, 0.5)\n",
        "IPython.display.Audio(\"./chime_output.wav\",autoplay=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejezNXgUNlQ5"
      },
      "source": [
        "# y_pred = np.where(model.predict(test_generator) > 0.5,1,0)\n",
        "# print(classification_report(Y_dev.flatten(),y_pred.flatten()))\n",
        "# print(confusion_matrix(Y_dev.flatten(),y_pred.flatten()))\n",
        "# print(f1_score(Y_dev.flatten(),y_pred.flatten(),average='binary'))\n",
        "\n",
        "# #TODO :: Fix this....."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-WhsFOhwQGR"
      },
      "source": [
        "f1.close()\n",
        "f2.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57xW3GSb6gv0"
      },
      "source": [
        "audio, sr = get_audio()\n",
        "scipy.io.wavfile.write('recording.wav', sr, audio)\n",
        "\n",
        "audio = AudioSegment.from_file('recording.wav', format=\"wav\", frame_rate=48000)\n",
        "audio = audio.set_frame_rate(16000)\n",
        "\n",
        "pad_ms = 10000  # Add here the fix length you want (in milliseconds)\n",
        "if pad_ms > len(audio):\n",
        "  silence = AudioSegment.silent(duration=pad_ms-len(audio),frame_rate=16000)\n",
        "  audio = audio + silence  # Adding silence after the audio\n",
        "\n",
        "audio.export('recording.wav', format=\"wav\")\n",
        "\n",
        "sr, audio = wav_read('recording.wav')\n",
        "audio = audio + 200     #Removing zeros from recording....\n",
        "scipy.io.wavfile.write('recording.wav', sr, audio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWFY1mKvO_Lq"
      },
      "source": [
        "prediction = detect_triggerword('recording.wav',new_model)\n",
        "chime_on_activate('recording.wav',chime_file, prediction, 0.5)\n",
        "IPython.display.Audio(\"./chime_output.wav\",autoplay=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRjZ0ahhWQ64"
      },
      "source": [
        "print(\"We Done\")\n",
        "#!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBT8pGBgWUxA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}